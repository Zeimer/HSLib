% Opcje klasy 'iithesis' opisane sa w komentarzach w pliku klasy. Za ich pomoca
% ustawia sie przede wszystkim jezyk i rodzaj (lic/inz/mgr) pracy, oraz czy na
% drugiej stronie pracy ma byc skladany wzor oswiadczenia o autorskim wykonaniu.
\documentclass[declaration,inz,english,shortabstract]{iithesis}

\usepackage[utf8]{inputenc}

%%%%% DANE DO STRONY TYTULOWEJ
% Niezaleznie od jezyka pracy wybranego w opcjach klasy, tytul i streszczenie
% pracy nalezy podac zarowno w jezyku polskim, jak i angielskim.
% Pamietaj o madrym (zgodnym z logicznym rozbiorem zdania oraz estetyka) recznym
% zlamaniu wierszy w temacie pracy, zwlaszcza tego w jezyku pracy. Uzyj do tego
% polecenia \fmlinebreak.
\englishtitle   {Formally verified programming with monads in Coq}
\polishtitle    {Formalnie zweryfikowane programowanie z monadami w Coqu}
\polishabstract {Prezentujemy \libname, Coqową bibliotekę do formalnie zweryfikowanego programowania z abstrakcjami w stylu Haskella: funktorami, funktorami aplikatywnymi, monadami, transformatorami monad oraz efektami opartymi o klasy typów. Omawiamy nasze decyzje projektowe i przedstawiamy działanie biblioteki na przykładach z \cite{JustDoIt}.}
\englishabstract{We introduce \libname, a Coq library for formally verified programming with Haskell-style abstractions: functors, applicative functors, monads, monad transformers and typeclass-based effects. We discuss the design choices we made and illustrate the working of the library by formalizing examples taken from \cite{JustDoIt}.}
% w pracach wielu autorow nazwiska mozna oddzielic poleceniem \and
\author         {Zeimer}
% w przypadku kilku promotorow, lub koniecznosci podania ich afiliacji, linie
% w ponizszym poleceniu mozna zlamac poleceniem \fmlinebreak
\advisor        {prof dr rehabilitowany Wpisuyashi TODO}
\date           {czerwiec 2019}                     % Data zlozenia pracy
% Dane do oswiadczenia o autorskim wykonaniu
%\transcriptnum {}                     % Numer indeksu
%\advisorgen    {dr. Jana Kowalskiego} % Nazwisko promotora w dopelniaczu
%%%%%

%%%%% WLASNE DODATKOWE PAKIETY
%
%\usepackage{graphicx,listings,amsmath,amssymb,amsthm,amsfonts,tikz}
%\usepackage{minted}
\usepackage{minted}
%
%%%%% WLASNE DEFINICJE I POLECENIA
%
%\theoremstyle{definition} \newtheorem{definition}{Definition}[chapter]
%\theoremstyle{remark} \newtheorem{remark}[definition]{Observation}
%\theoremstyle{plain} \newtheorem{theorem}[definition]{Theorem}
%\theoremstyle{plain} \newtheorem{lemma}[definition]{Lemma}
%\renewcommand \qedsymbol {\ensuremath{\square}}

\newcommand{\libname}{\textit{hsCoq}}

\newcommand{\defn}{:\equiv}
\newcommand{\m}[1]{\texttt{#1}}
\newcommand{\Prop}{\texttt{Prop}}
\newcommand{\Type}{\texttt{Type}}
\newcommand{\nat}{\texttt{nat}}
\newcommand{\Tree}{\texttt{Tree}}
%%%%%

\begin{document}

%%%%% POCZATEK ZASADNICZEGO TEKSTU PRACY

\chapter{Introduction}

This chapter gives some motivations for why formal verification of hardware, software and mathematics is useful and then briefly introduces the Coq proof assistant -- a tool for such verification -- to those who are not familiar with it.

The rest of the thesis is structured as follows:

\begin{itemize}
    \item In chapter 2 we discuss the problem of modeling computational effects in programming languages and compare existing approaches.
    \item In chapter 3 we present our library \libname and discuss its design.
    \item In chapter 4 we give some example effectful programs and prove their properties.
    \item In chapter 5 we describe our approach to proof engineering (the formalized mathematic's equivalent of software engineering) in our library.
    \item In chapter 6 we conclude and give some possible directions for further work.
\end{itemize}

\section{Formal verification of hardware and software}

Since their invention in the 1940s, computers' significane rose at a very fast pace. They were getting applied to an ever expanding range of problems by more and more people, private companies and governments alike. It shouldn't be a considered a surprise then that we became very reliant on them for both small conveniences and large scale projects.

But significane is not the only thing that rose -- another one is complexity. Exponentially growing processing speed required the complexity of chip designs to grow at a similar rate. More complex products and services require more complex software architectures and with new business models, like cloud computing, comes even more complexity in the form of virtualization, containerization and so on.

And with complexity comes, of course, the potential for bugs, which may cause a lot damage. A malfunction in software running the stock exchange can mean billions of dollars of losses; in software running a nuclear power plant -- deadly radiation for thousands of people and energy shortage for millions more.

Due to these dangers a lot of effort has been put into assuring that hardware and software are correct and with great success, but here and there bugs still have crept in. Some of the most spectecular were, recently:

\begin{itemize}
    \item Metldown, which ``exploits side effects of out-of-order execution on modern processors to read arbitrary kernel-memory locations including  personal  data  and  passwords.'' \cite{Meltdown}
    \item Spectre, which uses speculative execution and branch prediction to ``leak the victim's confidential information via a side  channel to  the  adversary.'' \cite{Spectre}
    \item Heartbleed, which ``allows stealing the information protected, under normal conditions, by the SSL/TLS encryption used to secure the Internet'' \cite{Heartbleed}
\end{itemize}

DeepSpec \cite{DeepSpec} is a Coq-based project that tries to eliminate both hardware and software bugs and security vulnerabilities by creating a web of formally verified hardware, operating systems, compilers, web servers, cryptography libraries etc.

\section{Formal verification of mathematics}

Hardware and software are not the only things in need of formal verification -- mathematics is also one of them.

The four colour theorem is a problem posed in 1852. It states that any planar map can be coloured with only four colours so that no two regions sharing a boundary are assigned the same colour. It became famous for resisting many proof attempts by many famous mathematicians for more than a century until it was finally proved by Appel and Haken in 1976. Its importance stems from the proof method -- it was the first major theorem proven using a computer program, whose job was to make sure a very large case analysis was exhaustive.

Thomas Tymoczko, a philosopher of mathematics, criticised this proof by labeling it with a term he invented just for this purpose -- ``non-surveyable''. He considered a proof to be non-surveyable when its verification cannot be performed by human mathematicians competent in the relevant field. Appel and Haken's proof certainly did fail the surveyability criterion -- the program was written in IBM 370 assembly, a language graph theorists very likely didn't understand.

This was the perfect theorem to let formal proofs and formally verified programs shine by dispeling Tymoczko's and other mathematician's doubts. This is what indeed happened in 2005, when Georges Gonthier presented a proof of the theorem formalized in Coq \cite{FourColour1} \cite{FourColour2}.

But big theorems with difficult proofs are not the only call for formalized mathematics. Another one is the mere fallibility of humans, especially their limited memory and reasoning skills and the tendency to follow authority. As unmathematical as it sounds, these are the main reasons cited by Vladimir Voevodsky, a field medalist mathematician turned a fan of formal proofs, in one of his talks \cite{UnivalentFoundations}.

The fields he refers to are homotopy theory, higher category theory and motivic cohomology -- all of them containing many layers of abstraction, tons of concepts and definitions, and rather shaky foundations.

As an example, noticing an error in one of his papers took 7 years and repairing the mistake another 6 years. In another, more extreme case, after publishing a paper in 1989, an alleged counterexample was found in 1998 by anoter expert in the field, but it was too difficult for them to agree on whether it really was a counterexample and Voevodsky only realized he was wrong in 2013. All of this put him in search of formalized foundations of mathematics, and he chose Coq to purse them. 

\section{The Coq proof assistant}

Coq \cite{Coq} is a proof assistant that was started in the late 1980's in France and still under active development. It consists of three languages:

\begin{itemize}
    \item Gallina, the term language, implements a formal system whose slight variants go under a plethora of names: Calculus of (Inductive) Constructions, (Intensional) Martin-L\"of Type Theory, Intuitionistic Type Theory, Constructive Type Theory, etc. We can use it to express specifications, programs, theorems and proofs.
    \item Vernacular, the command language, is a language of commands, which allow things like looking up useful theorems in the environment or creating modules.
    \item Ltac, the tactic language, is a language which facilitates writing proofs -- these can in principle be written using the term language, but it's unwieldy even for simple proofs.
\end{itemize}

The basic objects of our interest in Coq (and in any kind of the aforementioned formal systems) are types. Their role is to classify terms -- for example, $\m{21} + \m{21}$ is a term of type \m{nat}, written $\m{21} + \m{21} : \m{nat}$. Types are syntactical entities, which means that the judgement $\m{x} : \m{A}$ can always be checked algorithmically.

Thanks to the the Curry-Howard correspondence \cite{CurryHoward}, types can seen both as specifications of programs and as statements of theorems. All programming and proving may be then conceptualized as manipulating a few kinds of rules:

\begin{itemize}
    \item Formation rules tell us what types are there.
    \item Introduction rules tell us how to construct inhabitants of a given type.
    \item Elimination rules tell us how to use an inhabitant of some type to construct inhabitants of other types.
    \item Computation rules tell us how an elimination rule acts on an introduction rule -- computation happens when we first build something and then take it apart.
    \item Uniqueness rules tell us how an introduction rule acts on an elimination rule -- if we first take something apart and then rebuild it, we should get the same thing we initially had.
\end{itemize}

How does this play out in practice? Let's take a look at an example development, which illustrates the basic workings of Coq. At this point, we strongly encourage the unfamiliar reader to install CoqIDE (a dedicated IDE for Coq, available from \cite{Coq}) and run this snippet (which can be found in the thesis sources in the directory \m{snippets/}) in interactive mode -- this experience will be better than any explanation.

\definecolor{CoqIDE}{RGB}{255,248,220}

\begin{inputminted}
[
    frame=lines,
    bgcolor=CoqIDE,
    linenos
]
{Coq}{snippets/snippet1.v}
\end{inputminted}

A Coq file consists of a series of commands, which define types and terms, import and export modules, state or look up theorems etc. In this file we want to implement a function that labels the leaves of a tree with natural numbers starting from $0$.

For this, we will need the type $\m{nat}$, which is provided by the standard library. We can print it's definition using the command $\m{Print}$. What we get is an inductive definition of a type with two constructors, $\m{O}$ and $\m{S}$. $\m{O}$ is supposed to represent $0$ and $\m{S}$ is supposed to represent the successor operation, so that $\m{S}\ \m{O}$ represents $1$, $\m{S}\ (\m{S}\ \m{O})$ represents $2$ and so on.

We will also need a type $\m{Tree}$ that represents trees. The definition will be inductive, just as for natural numbers, but this time is has a parameter $A : \m{Type}$ that tells us the type of elements that can be stored in the trees. This can be seen as defining infinitely many types at once -- one for each choice of $A$. There are two constructors: $\m{Leaf}$, which represents a tree with one element, and $\m{Node}$, which represents a tree with two subtrees. Thus our trees are always nonempty and they contain elements only in their leaves.

The next two commands, \m{Arguments}, control the use of implicit arguments. Normally, we would have to write $\m{Leaf}\ \nat\ 42$ for the tree containing only the value $42$, but this is redundant, since given a term like $\m{42}$, we acan infer its type to be $\nat$. We can thus write this as $\m{Leaf}\ \_\ \m{42}$, but this is still redundant. Thanks to the first of these two commands, we can write simply $\m{Leaf}\ \m{42}$.

The function \m{label} is the clou of the whole development. It's type says that given a type \m{A} (which we won't need to give explicitly, since it's an implicit argument), a tree \m{t} and a number \m{n} which acts as the counter, it returns a pair consisting of a natural number (the new counter) and a tree contaning elements of type $\m{A} * \nat$.

The definition is recursive (marked by the keyword \m{Fixpoint}). When we hit a \m{Leaf}, we return the current counter and label the leaf with the counter's value. When we encounter a \m{Node}, we label the left subtree using the current counter, then label the right subtree with the new counter incremented by one, and then return the new counter and the two labeled subtrees joined by the constructor \m{Node}.

We can use this auxiliary function to define \m{lbl}, the function we wanted from the beginning. The command \m{Compute} normalizes a given term. In our case, we run \m{lbl} on an example tree to see that the result is as we expected -- the leaves are now labeled with natural numbers, starting at zero, from left to right. But how do we prove that what we have done really is correct?

The basic approach is to invent some properties we would like our constructions to satisfy. If they do satisfy all of these desired properties, we say they are correct.

In our case, one simple property is that after labeling a tree \m{t} the counter increases by the size of the tree minus one. To state this property formally, we first need to define a function that computes the size of a tree. We do this by recursion: a \m{Leaf} has size $1$ and the size of a \m{Node} is the sum of sizes of its subtrees.

The command \m{Require Import} imports a module. To prove our theorem, we will need some theorems living in a module called \m{Arith}.

Now we are ready to state our theorem. It reads: for any type \m{A}, any \m{t} which is a tree of elements of type \m{A}, any two natural numbers \m{n} and \m{n'} and any \m{t'} which is a tree of elements of type \m{A * nat}, if calling label on \m{t} and \m{n} results in the pair \m{(n', t')}, then the successor of \m{n'} is equal to \m{n + size\ t}.

There are some points to be noticed. First, we quantify over types, which means that our logic is not first-order, but higher-order. Second, we need to quantify over \m{t'}, even though the tree resulting from the call to label is on no interest to us. Third, we avoid using subtraction or the predecessor function and instead express our theorem using successor. This is a technicality -- our theorem will be a bit easier to prove.

We now enter the proof mode, in which we can use tactics to prove our theorem. The command \m{Proof} is useless, because it does nothing, but it looks nice at the beginning of a proof.

We proceed by induction on \m{t}. The clause \m{as [| l IHl r IHr]} allows us to name variables and induction hypotheses. This may seem insignificant, but is actually an important aspect of proof engineering -- to make our proofs readable and resilient to change, it's a good practice to name our things manually (as opposed to using automatically generated names).

To boost the unfamiliar reader's curiosity and encourage him to see the proof in CoqIDE, we will not go over it in detail. It suffices to say that the in first case the result holds almost by computation, but we need to use the commutativity of addition and in the second case we basically just use our two inductive hypotheses and the associativity of addition.

This concludes our introduction to the Coq proof assistant. From now on we will assume that the reader is familiar with it. If that's not the case, there are a great many of books which make a great introduction:

\begin{itemize}
    \item Software Foundations \cite{SoftwareFoundations} is a four volume book by many authors. The first volume treats the basics of Coq and the third one is about formally proving correctness of algorithms in Coq. The second and fourth volumes are less relevant, as they are about formalizing programming languages and randomized testing, respectively.
    \item Coq'Art \cite{CoqArt} is a comprehensive but a little dated (2004) book that covers way more material than Software Foundations, including coinduction, proof by reflection, case studies and so on.
    \item Certified Programming with Dependent Types \cite{CPDT} is an advanced book that focuses on using dependent types and proof engineering, but also covers topics like axioms, reasoning about equality proofs and general recursion.
\end{itemize}

\chapter{Effects}

\section{Basic concepts}

Although \libname\ is a rather general-purpose library, it also tackles the problem of how to best express (and program with) effects. Because it is a very hot topic recently, let's take a deeper look at it.

A \textbf{side effect} is some kind of communication with the outside world, like:

\begin{itemize}
    \item Reading external configuration.
    \item Logging to a file.
    \item Randomness, because it requires a seed.
    \item Any kind of IO, like connecting to the Internet or querying a database.
    \item Memory management, like allocation and freeing.
    \item Concurrency and threads, because they need some kind of synchronization, either through shared memory or message passing.
\end{itemize}

An \textbf{effect} (sometimes also called \textbf{computational effect}) is either a side effect or use of some control mechanism, like:

\begin{itemize}
    \item Generators and coroutines.
    \item Nondeterminism: the computation may return multiple results.
    \item Partiality: the computation may return \m{null} or some similar value representing the lack of result.
    \item Exceptions: the computation may fail by throwing an exception, which may or may not then be handled by some other computation.
    \item Goto: the computation may pass control to some other computation.
    \item Continuations: the most general control mechanism, which can express all of the above ones.
    \item Sometimes nontermination and even termination are also considered effects, but that's a topic for philosophical debate, beyond the scope of this thesis.
\end{itemize}

It should be quite obvious that effects are useful. They are in fact the main reason for running most computer programs and without them running even the simplest number-crunching program would be pointless, since without IO operations it couldn't tell us the result. We thus want to be able to express effects. But we also want to be able to reason about our effectful programs, which in most programming languages is notoriously difficult, and moreover we want to verify our resaoning.

\section{A comparison}

To better understand the matter, we can classify all expressions in a programming language as being either values or computations. A \textbf{value} necessarily has no effects whereas a \textbf{computation} may have effects. Philosophically, we say that a value is but a computation does. We can then say that a language is \textbf{pure} when it explicitly separates values from computations and \textbf{impure} when it does not.

When interpreted as a yes-no property, most languages are impure, so it is better to see the concept of purity as a continuum, where some languages may be more pure than others. Let's examine a few real-world languages and see, where they can be placed on this continuum.

C is a very impure language. There is global state in the form of global variables. Even though ``considered harmful'', we can use the \m{goto} statement anywhere, just as IO operations. It's fair to say that C certainly does not even try to separate values from computations.

In Java, there's no global state, but objects still have internal state that methods can change. There's no \m{goto} and some exceptions are checked, which makes them apparent in type signatures. But there also are unchecked exceptions that don't appear in type signatures and IO operations may be used anywhere. Thus, even though there are some ad hoc attempts at separating values from computations (checked exceptions), Java still can't do this in a principled way. It is still an impure language, though less than C.

Haskell, on the other hand, is a reasonably pure language. There is no global state. The impure error mechanism is unused and usually replaced with something more pure. IO's purity can be broken by using functions like \m{unsafePerformIO}, but this can't be done accidentally and so most IO is pure. All effects (besides the aforementioned unsafe IO and errors) are expressed using monads, which are clearly visible in type signatures. This makes values and computations very easy to tell apart.

At last, Koka \cite{Koka} is one of the purest languages in existence. It clearly separates values from computations -- each expression, besides its type, is also assigned the list of all effects that it may produce. Even nontermination is tracked this way.

To better see these differences, let's look at a piece of code that loops before trying to print something to the screen in each of these languages.

\begin{listing}[H]
    \begin{minted}{C}
        void f()
        {
            f();
            printf("Effectful\n");
        }
    \end{minted}
    \caption{C}
\end{listing}

\begin{listing}[H]
    \begin{minted}{Java}
        public void f()
        {
            f();
            System.out.println("Effectful");
        }
    \end{minted}
    \caption{Java}
\end{listing}

\begin{listing}[H]
    \begin{minted}{Haskell}
        f :: IO ()
        f = do
            f
            putStrLn "Effectful"
    \end{minted}
    \caption{Haskell}
\end{listing}

\begin{listing}[H]
    \begin{minted}{Koka}
        function f() : <div, io> ()
        {
            f()
            println("Effectful")
        }
    \end{minted}
    \caption{Koka}
\end{listing}


\section{Approaches to effect systems}

We already saw two approaches to managing effects. One, represented by C and Java, simply ignores the matter. The other one, represented by Haskell and Coq, tracks effect information using types.

The most recent and promising way, however, is the third approach, represented by languages such as Eff, Frank, Koka and Helium. It boils down to having a \textbf{type and effect system}, in which every expressions is associated with a type and a list or set of effects that it may cause. Values are then those expressions where this list/set is empty and computations are all the rest. 



At long last, Coq is a completely pure language. This is not because of some ingenuity on the party of its creators -- it's just that Coq has no IO and, more generally, no way to communicate with the outside world. This shouldn't surprise us -- as a proof assistant it's main purpose is to prove theorems, not to write programs interacting with databases or the Internet.

Because of this, it may seem that we don't need a way to express effects in Coq. After all, we can't make any use of them, right? Not at all. First, even though in Coq we can't interact with the outside world, we may want to model programs or programming languages that can. Second, remember that effects also encompass control mechanisms, and these are useful for our purposes. The function \m{label} we saw at the end of the previous chapter could look way better if expressed using the state effect.
\chapter{Design}

\chapter{Examples}

\chapter{A case study in proof engineering}

\chapter{Conclusion}

%%%%% BIBLIOGRAFIA

\begin{thebibliography}{1}

    \bibitem{JustDoIt}
        Jeremy Gibbons and Ralf Hinze, \\
        \textit{Just do It: Simple Monadic Equational Reasoning}, 2011 \\
        \url{http://www.cs.ox.ac.uk/jeremy.gibbons/publications/mr.pdf}

    \bibitem{Meltdown}
        Moritz Lipp, Michael Schwarz, Daniel Gruss, Thomas Prescher, Werner Haas, Anders Fogh, Jann Horn, Stefan Mangard, Paul Kocher, Daniel Genkin, Yuval Yarom and Mike Hamburg, \\
        \textit{Meltdown: Reading Kernel Memory from User Space}, 2018 \\
        \url{https://meltdownattack.com/meltdown.pdf}

    \bibitem{Spectre}
        Paul Kocher, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, Moritz Lipp, Stefan Mangard, Thomas Prescher, Michael Schwarz and Yuval Yarom, \\
        \textit{Spectre Attacks: Exploiting Speculative Execution}, 2019 \\
        \url{https://spectreattack.com/spectre.pdf}
    
    \bibitem{Heartbleed}
        \url{https://heartbleed.com},
        2019

    \bibitem{DeepSpec}
        \url{https://deepspec.org},
        2019

    \bibitem{FourColour1}
        Georges Gonthier,
        \textit{A computer-checked proof of the Four Colour Theorem}, 2005 \\
        \url{https://www.cl.cam.ac.uk/~lp15/Pages/4colproof.pdf}
    
    \bibitem{FourColour2}
        Georges Gonthier,
        \textit{Formal Proof -- The Four-Color Theorem}, 2008, \\
        \url{http://www.ams.org/notices/200811/tx081101382p.pdf}

    \bibitem{UnivalentFoundations}
        Vladimir Voevodsky,
        \textit{UNIVALENT FOUNDATIONS},
        slides for a talk given at IAS on 26 March 2014, \\
        \url{http://www.math.ias.edu/~vladimir/Site3/Univalent_Foundations_files/2014_IAS.pdf}

    \bibitem{Coq}
        \url{https://coq.inria.fr/}

    \bibitem{CurryHoward}
        Morten Heine Sørensen, Paweł Urzyczyn, \\
        \textit{Lectures on the Curry-Howard Isomorphism}, 2006
    
    \bibitem{SoftwareFoundations}
        Benjamin C. Pierce, Andrew W. Appel and many others, \\
        \textit{Software Foundations}, 2019, \\
        \url{https://softwarefoundations.cis.upenn.edu/}
    
    \bibitem{CoqArt}
        Yves Bertot and Pierre Castéran, \\
        \textit{Interactive Theorem Proving and Program Development \\ Coq'Art: The Calculus of Inductive Constructions}, 2004, \\
        \url{https://www.labri.fr/perso/casteran/CoqArt/}

    \bibitem{CPDT}
        Adam Chlipala,
        \textit{Certified Programming with Dependent Types}, 2019,
        \url{http://adam.chlipala.net/cpdt/}

    \bibitem{Koka}
        Daan Leijen,
        \textit{Koka: Programming with Row Polymorphic Effect Types}, 2014, \\
        \url{https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/paper-20.pdf}
\end{thebibliography}

%\begin{thebibliography}{1}
%\bibitem{example} \ldots
%\end{thebibliography}

\end{document}